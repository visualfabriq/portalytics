{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we start by importing modules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vf_portalytics.model import PredictionModel\n",
    "from vf_portalytics.dataset import DataSet\n",
    "from vf_portalytics.tool import create_train_test_sets, score_model, describe_columns\n",
    "from sklearn import linear_model, ensemble, svm, tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bkcharts import Histogram, Bar, Line, Scatter\n",
    "from bkcharts.attributes import cat\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we load data and create the basics\n",
    "# dataset = DataSet('ulnl_prod_20170621.msgpack', path='/home/carst/')\n",
    "# df = dataset.data_df\n",
    "df = pd.read_msgpack('/home/carst/ulnl_prod_20170627_total.msgpack')\n",
    "df['product_prodh_long2'] = df['product_prodh1'] + df['product_prodh2']\n",
    "df['second_placement_yn'] = df['second_placement_yn'].astype(np.bool)\n",
    "df['product_volume_per_cu'] = df['baseline_vol'] /  df['baseline_units']\n",
    "mask = (df['second_placement_perc'] < 40.0)\n",
    "df.loc[mask, 'second_placement_yn'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# describe available columns\n",
    "generic, field, tag, promotion_dimension, product_dimension, media, media_attr = describe_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove version fields\n",
    "remove_list = [x for x in df.columns if x[-1:] in ['2', '3', '5'] \n",
    "               and not x.startswith('promotion_dimension')\n",
    "               and not x.startswith('product_')\n",
    "               and not x.startswith('media')\n",
    "               and not x.startswith('field')\n",
    "               and not x.startswith('week_agg')\n",
    "               and not x.startswith('total_units')\n",
    "              ] + [x for x in df.columns if x.startswith('fwb') or x.startswith('total_vol') or x == 'internal']\n",
    "df = df[[x for x in df.columns if x not in remove_list]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove unused dimensions\n",
    "keep_dimension_list = [7, 9, 10, 13, 14, 16, 17, 18, 19, 20, 21, 22, 24, 74, 115, 122, 134, 136, 137, 138, 144]\n",
    "keep_list = [x for x in df.columns if \n",
    "            (not x.startswith('promotion_dimension') and not x.startswith('product_dimension'))\n",
    "              or int(x.split('_')[-1]) in keep_dimension_list]\n",
    "remove_list = [x for x in df.columns if x not in keep_list]\n",
    "# print(remove_list)\n",
    "df = df[keep_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove unused tags\n",
    "keep_tags_list = []\n",
    "remove_list = [x for x in df.columns if x.startswith('tag_') and x not in keep_tags_list]\n",
    "df = df[[x for x in df.columns if x not in remove_list]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove unused product attributes\n",
    "keep_list = ['product_1_mco', 'product_2_div', 'product_2b_div', \n",
    "             'product_3_cat', 'product_5_bet_nl', \n",
    "             'product_6_bc', 'product_7_cc', 'product_7a_ebf_code', \n",
    "             'product_8_csc', 'product_8a_spf_code', \n",
    "             'product_9a_spfv_code', \n",
    "             'product_bonus', \n",
    "             'product_brandcategorykey', 'product_brandformkey', 'product_brandkey', 'product_brandmarketkey', \n",
    "             'product_brandsubsectorkey', 'product_brandtotalkey',\n",
    "             'product_foodsolutions', 'product_ho_brand_caratbrandkey', \n",
    "             'product_level_01', 'product_level_02', \n",
    "             'product_ntgew', \n",
    "             'product_prodh', 'product_prodh1', 'product_prodh2', 'product_prodh3', 'product_prodh_long2',\n",
    "             'product_repack_type',\n",
    "             'product_volume_per_cu'\n",
    "            ]\n",
    "\n",
    "remove_list = [x for x in df.columns if x.startswith('product_') and not x.startswith('product_dimension') and x not in keep_list]\n",
    "df = df[[x for x in df.columns if x not in remove_list]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add manual exclusions\n",
    "mask = df['promotion_ext_id'].isin([15148])\n",
    "df.loc[mask, 'exclude_yn'] = True\n",
    "\n",
    "# remove exclusions\n",
    "exclusion_mask = df['exclude_yn'].isin([1, True])\n",
    "df = df[-exclusion_mask]\n",
    "print(\"\\nNr of Excluded Records Removed: %d.\" % (exclusion_mask.sum()))\n",
    "\n",
    "# save the future\n",
    "status_mask = df['promotion_status'] < 110\n",
    "future_df = df[status_mask]\n",
    "print(\"\\nNr of Future Records: %d rows with %d features.\" % (future_df.shape[0], future_df.shape[1]))\n",
    "\n",
    "date_mask = future_df['yearweek'] > 201730\n",
    "future_df = future_df[date_mask]\n",
    "print(\"\\nNr of Future Records after week filter: %d rows with %d features.\" % (future_df.shape[0], future_df.shape[1]))\n",
    "\n",
    "baseline_mask = future_df['baseline_units'] > 0.0\n",
    "future_df = future_df[baseline_mask]\n",
    "print(\"\\nNr of Future Records after baseline filter: %d rows with %d features.\" % (future_df.shape[0], future_df.shape[1]))\n",
    "\n",
    "# filtering the contents\n",
    "status_mask = df['promotion_status'] >= 112\n",
    "df = df[status_mask]\n",
    "print(\"\\nAfter status filter: We have %d df with %d features.\" % (df.shape[0], df.shape[1]))\n",
    "\n",
    "date_mask = df['yearweek'] >= 201612\n",
    "df = df[date_mask]\n",
    "print(\"\\nAfter week filter: %d rows with %d features.\" % (df.shape[0], df.shape[1]))\n",
    "\n",
    "small_df_mask = df['baseline_units'] > 0.0\n",
    "df = df[small_df_mask]\n",
    "print(\"\\nAfter >0 baseline filter: %d rows with %d features.\" % (df.shape[0], df.shape[1]))\n",
    "\n",
    "huge_df_mask = df['baseline_units'] < 10.0**5\n",
    "df = df[huge_df_mask]\n",
    "print(\"\\nAfter <10k baseline filter: %d rows with %d features.\" % (df.shape[0], df.shape[1]))\n",
    "\n",
    "small_lift_mask = (df['lift'] > 1.2) & (df['lift'] < 40)\n",
    "df = df[small_lift_mask]\n",
    "print(\"\\nAfter lift filter: %d rows with %d features.\" % (df.shape[0], df.shape[1]))\n",
    "\n",
    "min_discount_mask = (df['discount_perc'] >= 5.0) & (df['discount_perc'] < 75.0)\n",
    "df = df[min_discount_mask]\n",
    "print(\"\\nAfter discount filter: %d rows with %d features.\" % (df.shape[0], df.shape[1]))\n",
    "\n",
    "mechanism_msk = df['mechanism'].notnull()\n",
    "df = df[mechanism_msk]\n",
    "print(\"\\nAfter mechanism filter: %d rows with %d features.\" % (df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Most Relevant Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and prepare data\n",
    "feature_df = df.select_dtypes(exclude=[np.object]).dropna(axis=1).copy()\n",
    "# Turn textual columns and booleans into classes (PredictionModel does this automatically in the preparation later) \n",
    "for column in feature_df.select_dtypes(include=[np.bool]):\n",
    "    feature_df[column] = LabelEncoder().fit_transform(feature_df[column])\n",
    "feature_col_list = [x for x in feature_df.columns if x != 'lift' and not x.startswith('total_units')]\n",
    "\n",
    "max_features = len(feature_col_list)\n",
    "\n",
    "print('# of numeric features available: ' + str(max_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set your parameters; please be aware that mutual_info_regression can be very resource intensive\n",
    "features_len = 30  # max number of columns: 'all' or a number\n",
    "selected_regression = f_regression  # f_regression (univariate) or mutual_info_regression (mutual information)\n",
    "\n",
    "if features_len > max_features:\n",
    "    features_len = max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check support\n",
    "features = SelectKBest(selected_regression, k=features_len).fit(feature_df[feature_col_list], feature_df['lift'])\n",
    "feature_support = features.get_support()\n",
    "max_score = max(features.scores_)\n",
    "if max_score == 0.0 or max_score != max_score:\n",
    "    max_score = 1.0\n",
    "feature_columns = {col: (100.0 * score / max_score) for col, selected, score in zip(list(feature_df[feature_col_list]), feature_support, features.scores_) if selected}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The ' + str(features_len) + ' most important features are (in descending order): \\n')\n",
    "\n",
    "display(HTML(\n",
    "    '<table><tr>{}</tr></table>'.format(\n",
    "        '</tr><tr>'.join(\n",
    "            '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in sorted(feature_columns.items(), key=lambda kv: kv[1], reverse=True))\n",
    "        )\n",
    " ))\n",
    "print('\\nPlease do not forget to manually check for co-correlation (such as baseline_units and baseline_vol)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up and filter dataframe to relevant columns (selected + standard)\n",
    "del feature_df\n",
    "# filter_cols = list(set(feature_columns.keys() + generic + field))\n",
    "# df = df[filter_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you want to filter the data set to these columns:\n",
    "# dataset.data_df = df\n",
    "# dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Automated Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude standard columns\n",
    "exclude_list = ['promotion_name', 'promotion_id', 'promotion_ext_id', 'account_banner', 'epid', 'ep_desc', 'baseline_units_ext', 'baseline_units_int', 'yearweek', 'total_units', 'total_units_2', 'total_units_3', 'total_units_5']\n",
    "customer_exclude_list = ['baseline_vol', 'total_baseline_vol', 'weighted_distribution_normal', 'weighted_distribution_promoted']\n",
    "\n",
    "# clean and prepare data\n",
    "feature_df = df.copy()\n",
    "\n",
    "# fill numeric values with 0.0\n",
    "for column in feature_df.select_dtypes(include=[np.int64, np.float64]):\n",
    "    feature_df[column] = feature_df[column].fillna(0.0)\n",
    "\n",
    "for column in feature_df.select_dtypes(include=[np.int64, np.float64]):\n",
    "    feature_df[column] = feature_df[column].fillna(0.0)\n",
    "\n",
    "# Turn textual columns and booleans into classes (PredictionModel does this automatically in the preparation later) \n",
    "for column in feature_df.select_dtypes(include=[np.object, np.bool]):\n",
    "    feature_df[column] = LabelEncoder().fit_transform(feature_df[column].fillna(-1))\n",
    "feature_col_list = [x for x in feature_df.columns if x != 'lift' and x not in exclude_list and x not in customer_exclude_list]\n",
    "\n",
    "max_features = len(feature_col_list)\n",
    "\n",
    "print('# of total features available: ' + str(max_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your parameters; please be aware that mutual_info_regression can be very resource intensive\n",
    "features_len = 5  # max number of columns: 'all' or a number\n",
    "step = 1  # x features to be dropped each step\n",
    "\n",
    "if features_len > max_features:\n",
    "    features_len = max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ensemble.ExtraTreesRegressor()\n",
    "\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model,  n_features_to_select=features_len, step=step, verbose=1)\n",
    "rfe = rfe.fit(feature_df[feature_col_list], feature_df['lift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The order of importance of the features is: \\n')\n",
    "feature_columns = {col: ranking for col, selected, ranking in \n",
    "                   zip(list(feature_df[feature_col_list]), rfe.support_, rfe.ranking_) \n",
    "                   # if selected\n",
    "                  }\n",
    "\n",
    "col_list = sorted(feature_columns.items())\n",
    "\n",
    "display(HTML(\n",
    "    '<table><tr>{}</tr></table>'.format(\n",
    "        '</tr><tr>'.join(\n",
    "            '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in sorted(feature_columns.items(), key=lambda kv: kv[1]))\n",
    "        )\n",
    " ))\n",
    "print('\\nPlease do not forget to manually check for co-correlation (such as baseline_units and baseline_vol)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up and filter dataframe to relevant columns (selected + standard)\n",
    "del feature_df\n",
    "# filter_cols = list(set(feature_columns.keys() + generic + field))\n",
    "# df = df[filter_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you want to filter the data set to these columns:\n",
    "# dataset.data_df = df\n",
    "# dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distribution histograms \n",
    "check_list = ['discount_perc', 'account_banner', 'week', 'baseline_units', 'field_35060']\n",
    "\n",
    "# nb: we filter out the bottom and top 0.5%)\n",
    "low_limit_def = 0.0\n",
    "high_limit_def = 100.0\n",
    "# nb: normal histograms are for numeric columns only, others are top 20 bar charts \n",
    "top_def = 20\n",
    "df['count'] = 1  # nb: the count is the promotion - product combinations\n",
    "\n",
    "# now plot\n",
    "for col in check_list:\n",
    "    if df[col].dtype in [np.int64, np.float64]:\n",
    "        low_limit = np.percentile(df[col].fillna(0.0), low_limit_def)\n",
    "        high_limit = np.percentile(df[col].fillna(0.0), high_limit_def)\n",
    "        print('Showing ' + col + ' between ' + str(low_limit) + ' and ' + str(high_limit))\n",
    "        mask = (df[col] >= low_limit) & (df[col] <= high_limit)\n",
    "        p = Histogram(df[mask], values=col, bins=12)\n",
    "        p.axis[1].axis_label = 'Count'\n",
    "        show(p)\n",
    "    else:\n",
    "        print('Showing ' + col + ' top ' + str(top_def))\n",
    "        group = df.groupby([col], as_index=False)['count'].sum()\n",
    "        group[col] = group[col].str.encode('utf-8')\n",
    "        group = group.nlargest(top_def, 'count')\n",
    "        label = cat(columns=col, sort=False)\n",
    "        p = Bar(group, label=label, values='count', legend=None)\n",
    "        p.axis[1].axis_label = 'Count'\n",
    "        show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# charts that check against lift\n",
    "check_list = ['discount_perc', 'account_banner', 'field_35060']\n",
    "\n",
    "# now plot\n",
    "for col in check_list:\n",
    "    if df[col].dtype in [np.int64, np.float64]:\n",
    "        print('Drawing Average Lift + Nr Observations for ' + col)\n",
    "        df['tmp_col'] = df[col].round() \n",
    "        group = df.groupby(['tmp_col'], as_index=False)['baseline_units', 'total_units', 'count'].sum()\n",
    "        del df['tmp_col']\n",
    "        group['lift'] = group['total_units'] / group['baseline_units']\n",
    "        group = group.rename(columns={'tmp_col': col})\n",
    "        p = Line(group, x=col, y='lift')\n",
    "        show(p)\n",
    "        p = Line(group, x=col, y='count', color='green')\n",
    "        show(p) \n",
    "    else:\n",
    "        print('Showing ' + col + ' Average Lift for ' + str(top_def) + ' most used')\n",
    "        group = df.groupby([col], as_index=False)['baseline_units', 'total_units', 'count'].sum()\n",
    "        group[col] = group[col].str.encode('utf-8')\n",
    "        group = group.nlargest(top_def, 'count')\n",
    "        group['lift'] = group['total_units'] / group['baseline_units']\n",
    "        label = cat(columns=col, sort=False)\n",
    "        p = Bar(group, label=label, values='lift', legend=None)\n",
    "        p.axis[1].axis_label = 'Avg Lift'\n",
    "        show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot scatter diagrams for correlation visualization\n",
    "check_list = [('discount_perc', 'lift'), ('baseline_units', 'total_units'), ('total_units_3', 'total_units_5')]\n",
    "\n",
    "# now plot\n",
    "for col_x, col_y in check_list:\n",
    "    if df[col_x].dtype in [np.int64, np.float64] and df[col_y].dtype in [np.int64, np.float64]:\n",
    "        print('Drawing Scatter Correlation for ' + col_x + ' and ' + col_y)\n",
    "        p = Scatter(df, x=col_x, y=col_y)\n",
    "        show(p)\n",
    "    else:\n",
    "        print('Both columns need to be numerical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model and select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a prediction model\n",
    "prediction_model = PredictionModel('carst_example', path='/home/carst/')\n",
    "\n",
    "# set the features (C = categoric value for dimensional features)\n",
    "prediction_model.features = {\n",
    "    'promoted_price': [],\n",
    "    'discount_perc': [],\n",
    "    'discount_amt': [],\n",
    "    'baseline_units': [],\n",
    "    'total_nr_products': [],  # total to check the complete size of the promotion\n",
    "    'total_baseline_units': [],  # total to check the complete size of the promotion\n",
    "    'account_id': ['C'],  # account\n",
    "    'field_35060': ['C'],  # Promotion Mechanism \n",
    "    'product_prodh1': ['C'],\n",
    "    'product_prodh_long2': ['C'],\n",
    "    'product_prodh': ['C'],\n",
    "    'multi_buy_y': ['C'],\n",
    "    'second_placement_yn': ['C'],\n",
    "    'week_agg_2': ['C'],\n",
    "    'product_volume_per_cu': [],  # check\n",
    "    }\n",
    "\n",
    "# we predict the lift normally or log?\n",
    "prediction_model.target = {'lift': []}  # [] or ['log']\n",
    "\n",
    "# are we doing logarithmic prediction?\n",
    "if 'log' in prediction_model.target['lift']:\n",
    "    log = True\n",
    "else:\n",
    "    log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mask = df['promotion_ext_id'] == 20382  # 22171\n",
    "df[mask][['epid', 'ep_desc', 'second_placement_yn','second_placement_perc', 'weighted_distribution_normal', 'weighted_distribution_promoted', 'baseline_units', 'total_units_5', 'lift']].sort_values('baseline_units', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get only use the needed columns\n",
    "used_column_list = list(set(prediction_model.features.keys() + prediction_model.target.keys()))\n",
    "\n",
    "# create a mask based on random selections or on a period\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "df['train_test'] = 'train'\n",
    "df.loc[-mask, 'train_test'] = 'test'\n",
    "\n",
    "# create train sets\n",
    "train_df, train_lift, test_df, test_lift = create_train_test_sets(df[used_column_list], mask, prediction_model, prediction_target='lift')\n",
    "\n",
    "# are we doing logarithmic predictions\n",
    "if log:\n",
    "    # we need to train everything based on the log value\n",
    "    train_lift = np.log(train_lift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create and train a regressor\n",
    "# regressor = svm.SVR()\n",
    "# regressor = ensemble.RandomForestRegressor(n_estimators=79, min_samples_split=4, , n_jobs=-1)\n",
    "regressor = ensemble.ExtraTreesRegressor(\n",
    "    n_estimators=100,\n",
    "    criterion='mse',  # mse or mae\n",
    "    max_depth=None,  # None or max nr for tree\n",
    "    min_samples_split=2, # standard 2; how significant nr of observations do we need for a split in the forest\n",
    "    min_samples_leaf=1, # standard 1; how significant nr of observations do we need for a split in the forest\n",
    "    max_leaf_nodes=None,  # standard None; how many nodes can we have in a leaf\n",
    "    # random_state=10, \n",
    "    n_jobs=-1)\n",
    "# regressor = ensemble.AdaBoostRegressor(ensemble.ExtraTreesRegressor(n_estimators=20, n_jobs=-1), n_estimators=79)\n",
    "# regressor = ensemble.GradientBoostingRegresor(n_estimators=5000, learning_rate=0.9, max_depth=10, random_state=0)\n",
    "\n",
    "regressor.fit(train_df, train_lift)\n",
    "prediction_model.model = regressor\n",
    "\n",
    "# possibly check: https://github.com/automl/auto-sklearn/blob/master/example/example_regression.py\n",
    "# https://mlwave.com/kaggle-ensembling-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and score the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict the lift\n",
    "predict_lift = prediction_model.model.predict(test_df)\n",
    "\n",
    "if log:\n",
    "    # if it was logarithmic, expand the lift again\n",
    "    predict_lift = np.exp(predict_lift)\n",
    "\n",
    "# score the model\n",
    "score_model(predict_lift, test_lift, baseline=test_df['baseline_units'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the feature importance\n",
    "feature_importance = regressor.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "mask = feature_importance > 0.5\n",
    "feature_importance = feature_importance[mask]\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "output_list = []\n",
    "for val, feature in zip(feature_importance[sorted_idx], train_df.columns[sorted_idx]):\n",
    "    output_list.append({'feature': feature, 'importance': val})\n",
    "importance_df = pd.DataFrame(output_list)\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# now plot a chart\n",
    "label = cat(columns='feature', sort=False)\n",
    "p = Bar(importance_df, label=label, values='importance', legend=None)\n",
    "p.axis[1].axis_label = 'Importance'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nb: we will now predict from the df\n",
    "test_df = df[df['train_test'] == 'test']\n",
    "test_df['lift_act'] = test_df['lift']\n",
    "\n",
    "# Unox example filter\n",
    "# mask = test_df['promotion_name'].str.lower().str.contains('plus')\n",
    "mask = test_df['ep_desc'].str.lower().str.contains('robijn')\n",
    "test_df = test_df[mask]\n",
    "\n",
    "predict_df = test_df[prediction_model.features.keys()].copy()\n",
    "predict_df = prediction_model.pre_processing(predict_df)\n",
    "predict_lift = prediction_model.model.predict(predict_df)\n",
    "if log:\n",
    "    # if it was logarithmic, expand the lift again\n",
    "    predict_lift = np.exp(predict_lift)\n",
    "test_df['lift_pred'] = predict_lift\n",
    "\n",
    "# check where we are off\n",
    "test_df['count'] = 1\n",
    "test_df['total_units_act'] = test_df['baseline_units'] * test_df['lift_act']\n",
    "test_df['total_units_pred'] = test_df['baseline_units'] * test_df['lift_pred']\n",
    "test_df['total_units_diff'] = test_df['total_units_act'] - test_df['total_units_pred']\n",
    "test_df['total_units_abs_diff'] = np.abs(test_df['total_units_diff'])\n",
    "test_df['total_units_perc_diff'] = test_df['total_units_abs_diff'] / test_df['total_units_pred']\n",
    "\n",
    "# add total average account/product lift\n",
    "total_df = test_df.groupby(['account_id', 'epid'], as_index=False)[['total_units_act', 'baseline_units']].sum()\n",
    "total_df['lift_act_product_avg'] = total_df['total_units_act'] / total_df['baseline_units']\n",
    "del total_df['total_units_act']\n",
    "del total_df['baseline_units']\n",
    "test_df = pd.merge(test_df, total_df, how='left')\n",
    "del total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# result: prediction vs actual -> plot + line etc. <- we will copy stuff from the investigation part\n",
    "# plot scatter diagrams for correlation visualization\n",
    "check_list = [('total_units_act', 'total_units_pred'), ('lift_act', 'lift_pred'), ('total_units_act', 'total_units_3')]\n",
    "\n",
    "# now plot\n",
    "for col_x, col_y in check_list:\n",
    "    if test_df[col_x].dtype in [np.int64, np.float64] and test_df[col_y].dtype in [np.int64, np.float64]:\n",
    "        print('Drawing Scatter Correlation for ' + col_x + ' and ' + col_y)\n",
    "        p = Scatter(test_df, x=col_x, y=col_y)\n",
    "        total_max = int(max(test_df[col_x].max(), test_df[col_y].max()))\n",
    "        line_range = list(range(total_max))\n",
    "        p.line(line_range, line_range)\n",
    "        show(p)\n",
    "    else:\n",
    "        print('Both columns need to be numerical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# charts that check against variance\n",
    "top_def = 20\n",
    "check_list = ['discount_perc', 'account_banner', 'promotion_dimension_144', 'promotion_dimension_7', 'field_35060', 'product_prodh_long2']\n",
    "\n",
    "# now plot\n",
    "for col in check_list:\n",
    "    if df[col].dtype in [np.int64, np.float64]:\n",
    "        print('Drawing Average Difference + Nr Observations for ' + col)\n",
    "        test_df['tmp_col'] = test_df[col].round() \n",
    "        group = test_df.groupby(['tmp_col'], as_index=False)['total_units_act', 'total_units_pred', 'count'].sum()\n",
    "        del test_df['tmp_col']\n",
    "        group['variance'] = group['total_units_pred'] / group['total_units_act'] * 100.0\n",
    "        group = group.rename(columns={'tmp_col': col})\n",
    "        p = Line(group, x=col, y='variance')\n",
    "        show(p)\n",
    "        p = Line(group, x=col, y='count', color='green')\n",
    "        show(p) \n",
    "    else:\n",
    "        print('Showing ' + col + ' Average Lift for ' + str(top_def) + ' most used')\n",
    "        group = test_df.groupby([col], as_index=False)['total_units_act', 'total_units_pred', 'count'].sum()\n",
    "        group[col] = group[col].str.encode('utf-8')\n",
    "        group = group.nlargest(top_def, 'count')\n",
    "        group['variance'] = group['total_units_pred'] / group['total_units_act'] * 100.0\n",
    "        label = cat(columns=col, sort=False)\n",
    "        p = Bar(group, label=label, values='variance', legend=None)\n",
    "        p.axis[1].axis_label = 'Total Variance'\n",
    "        show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# promotion product level differences\n",
    "mask = (test_df['total_units_abs_diff'] / test_df['total_units_act'] > 0.3)\n",
    "test_df[mask].sort_values('total_units_abs_diff', ascending=False)[0:100][['promotion_name', 'promotion_ext_id', 'ep_desc', 'second_placement_yn', 'baseline_units', 'total_units_pred', 'total_units_act', 'lift_pred', 'lift_act', 'lift_act_product_avg']][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# promotion level differences\n",
    "group_df = test_df.groupby(['promotion_name', 'promotion_ext_id'])[['total_units_act', 'total_units_pred', 'total_units_abs_diff']].sum()\n",
    "group_df.sort_values('total_units_abs_diff', ascending=False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (test_df['total_units_act'] > 200000) & (test_df['total_units_act'] < 300000) & (test_df['total_units_pred'] < 150000)\n",
    "test_df[mask][['promotion_name', 'promotion_ext_id', 'total_units_act', 'total_units_pred', 'total_units_abs_diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Future Promotions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = (future_df['promotion_ext_id'] == 29052) & (future_df['epid'].isin(['57c41eb57938e3ff051a9705', '57c41eb37938e3ff051a9682']))\n",
    "future_df.loc[mask, 'second_placement_yn'] = True\n",
    "\n",
    "predict_df = future_df.copy()\n",
    "\n",
    "# select only relevant columns\n",
    "predict_df = predict_df[prediction_model.features.keys()]\n",
    "\n",
    "predict_df = prediction_model.pre_processing(predict_df)\n",
    "predict_lift = prediction_model.model.predict(predict_df)\n",
    "if log:\n",
    "    # if it was logarithmic, expand the lift again\n",
    "    predict_lift = np.exp(predict_lift)\n",
    "\n",
    "future_df['lift_pred'] = predict_lift\n",
    "\n",
    "# check where we are off\n",
    "future_df['count'] = 1\n",
    "future_df['total_units_act'] = future_df['total_units']\n",
    "future_df['total_units_pred'] = future_df['baseline_units'] * future_df['lift_pred']\n",
    "future_df['total_units_diff'] = future_df['total_units_act'] - future_df['total_units_pred']\n",
    "future_df['total_units_abs_diff'] = np.abs(future_df['total_units_diff'])\n",
    "future_df['total_units_perc_diff'] = future_df['total_units_abs_diff'] / future_df['total_units_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = future_df['promotion_ext_id'] == 29052\n",
    "future_df[mask][['epid', 'ep_desc', 'second_placement_yn','baseline_units', 'total_units_pred', 'total_units_act', 'lift_pred', 'lift']].sort_values('baseline_units', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# result: prediction vs actual -> plot + line etc. <- we will copy stuff from the investigation part\n",
    "# plot scatter diagrams for correlation visualization\n",
    "check_list = [('baseline_units', 'total_units_pred'), ('total_units_act', 'total_units_pred')]\n",
    "\n",
    "# now plot\n",
    "for col_x, col_y in check_list:\n",
    "    if future_df[col_x].dtype in [np.int64, np.float64] and future_df[col_y].dtype in [np.int64, np.float64]:\n",
    "        print('Drawing Scatter Correlation for ' + col_x + ' and ' + col_y)\n",
    "        p = Scatter(future_df, x=col_x, y=col_y)\n",
    "        show(p)\n",
    "    else:\n",
    "        print('Both columns need to be numerical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = (future_df['promotion_ext_id'] == 34310)\n",
    "future_df[mask][['promotion_name', 'promotion_ext_id', 'baseline_units', 'total_units_act', 'total_units_pred', 'total_units_abs_diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df[mask].groupby(['promotion_name', 'promotion_ext_id'])[['baseline_units', 'total_units_act', 'total_units_pred', 'total_units_abs_diff']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "future_df.groupby(['promotion_name', 'promotion_ext_id', 'account_banner', 'yearweek'], as_index=False)[['baseline_units', 'total_units_act', 'total_units_pred', 'total_units_abs_diff']].sum().to_excel('/home/carst/ulnl_prediction_vs_actual.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save the model\n",
    "prediction_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python36] *",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
